{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from DataLoader import VideoQADataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jupyter/.netrc\n"
     ]
    }
   ],
   "source": [
    "!wandb login 3ed7a1bc59fad48beeadc999df34dbee428be831"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnicolas-dufour\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.12<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">jumping-dream-55</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/nicolas-dufour/video-qa-hcrn-recvis\" target=\"_blank\">https://wandb.ai/nicolas-dufour/video-qa-hcrn-recvis</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/nicolas-dufour/video-qa-hcrn-recvis/runs/3cteaknu\" target=\"_blank\">https://wandb.ai/nicolas-dufour/video-qa-hcrn-recvis/runs/3cteaknu</a><br/>\n",
       "                Run data is saved locally in <code>/home/jupyter/video-qa-recvis/wandb/run-20210102_015713-3cteaknu</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(3cteaknu)</h1><p></p><iframe src=\"https://wandb.ai/nicolas-dufour/video-qa-hcrn-recvis/runs/3cteaknu\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f0e786b3610>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"video-qa-hcrn-recvis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_dict(d):\n",
    "    return {v: k for k, v in d.items()}\n",
    "class VideoQADataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_path,dataset_name,batch_size,text_embedding_method):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.dataset_name = dataset_name\n",
    "        if(self.dataset_name == 'msvd-qa' or self.dataset_name == 'msrvtt-qa'):\n",
    "            self.question_type = 'none'\n",
    "        elif(self.dataset_name == 'tgif-qa_frameqa'):\n",
    "            self.question_type = 'frameqa'\n",
    "        self.text_embedding_method = text_embedding_method\n",
    "        self.num_workers = 4\n",
    "        \n",
    "        self.dataset_path = f\"{data_path}/{self.dataset_name}\"\n",
    "        \n",
    "        if(self.text_embedding_method == 'glove'):\n",
    "            with open(f\"{self.dataset_path}/{self.text_embedding_method}_question_embedding/{self.dataset_name}_train_questions.pt\", 'rb') as f:\n",
    "                obj = pickle.load(f)\n",
    "                glove_matrix = obj['glove']\n",
    "            self.glove_matrix = glove_matrix\n",
    "        with open(f\"{self.dataset_path}/{self.dataset_name}_vocab_{self.text_embedding_method}.json\", 'r') as f:\n",
    "            vocab = json.load(f)\n",
    "            vocab['question_idx_to_token'] = invert_dict(vocab['question_token_to_idx'])\n",
    "            vocab['answer_idx_to_token'] = invert_dict(vocab['answer_token_to_idx'])\n",
    "            vocab['question_answer_idx_to_token'] = invert_dict(vocab['question_answer_token_to_idx'])\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return VideoQADataLoader(\n",
    "                question_type = self.question_type,\n",
    "                batch_size = self.batch_size,\n",
    "                num_workers = self.num_workers,\n",
    "                question_pt = f\"{self.dataset_path}/{self.text_embedding_method}_question_embedding/{self.dataset_name}_train_questions.pt\" ,\n",
    "                appearance_feat = f\"{self.dataset_path}/{self.dataset_name}_appearance_feat.h5\",\n",
    "                motion_feat = f\"{self.dataset_path}/{self.dataset_name}_motion_feat.h5\",\n",
    "                shuffle=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return VideoQADataLoader(\n",
    "                question_type = self.question_type,\n",
    "                batch_size = self.batch_size,\n",
    "                num_workers = self.num_workers,\n",
    "                question_pt = f\"{self.dataset_path}/{self.text_embedding_method}_question_embedding/{self.dataset_name}_val_questions.pt\" ,\n",
    "                appearance_feat = f\"{self.dataset_path}/{self.dataset_name}_appearance_feat.h5\",\n",
    "                motion_feat = f\"{self.dataset_path}/{self.dataset_name}_motion_feat.h5\",\n",
    "                shuffle=False\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return VideoQADataLoader(\n",
    "                question_type = self.question_type,\n",
    "                batch_size = self.batch_size,\n",
    "                num_workers = self.num_workers,\n",
    "                question_pt = f\"{self.dataset_path}/{self.text_embedding_method}_question_embedding/{self.dataset_name}_test_questions.pt\" ,\n",
    "                appearance_feat = f\"{self.dataset_path}/{self.dataset_name}_appearance_feat.h5\",\n",
    "                motion_feat = f\"{self.dataset_path}/{self.dataset_name}_motion_feat.h5\",\n",
    "                shuffle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "msvd_glove_data_module = VideoQADataModule('data','msvd-qa',batch_size=32,text_embedding_method='glove')\n",
    "msrvtt_glove_data_module = VideoQADataModule('data','msrvtt-qa',batch_size=32,text_embedding_method='glove')\n",
    "tgif_glove_data_module = VideoQADataModule('data','tgif-qa_frameqa',batch_size=32,text_embedding_method='glove')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model.HCRN as HCRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HCRN_glove(pl.LightningModule):\n",
    "    def __init__(self,glove_matrix,lr,model_kwargs,optimizer = 'AdamW'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.train_acc = pl.metrics.Accuracy()\n",
    "        self.valid_acc = pl.metrics.Accuracy()\n",
    "        self.test_acc = pl.metrics.Accuracy()\n",
    "        \n",
    "        glove_matrix = torch.FloatTensor(glove_matrix)\n",
    "        self.model = HCRN.HCRNNetwork(**model_kwargs)\n",
    "        with torch.no_grad():\n",
    "            self.model.linguistic_input_unit.encoder_embed.weight.set_(glove_matrix)\n",
    "    \n",
    "    def forward(self,ans_candidates, ans_candidates_len, video_appearance_feat, video_motion_feat, question,\n",
    "                question_len):\n",
    "        return self.model(ans_candidates, ans_candidates_len, video_appearance_feat, video_motion_feat, question,\n",
    "                question_len)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        if(self.optimizer == 'Adam'):\n",
    "            optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        elif(self.optimizer == 'AdamW'):\n",
    "            optimizer = optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        else:\n",
    "            raise \"Optimizer not supported\"\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer,10,gamma=0.5)\n",
    "        return [optimizer],[scheduler]\n",
    "   \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        _, _, answers, *batch_input = batch\n",
    "        logits = self(*batch_input)\n",
    "        loss = self.criterion(logits, answers)\n",
    "        acc = self.train_acc(logits,answers)\n",
    "        self.log('step_loss',loss,prog_bar = True,logger=False)\n",
    "        return {'loss': loss,'n_samples':len(answers)}\n",
    "    \n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        loss=0\n",
    "        n_samples = 0\n",
    "        for step_out in training_step_outputs:\n",
    "            loss += step_out['loss']\n",
    "            n_samples += step_out['n_samples']\n",
    "        loss = loss/n_samples\n",
    "        self.log('train_loss',loss,logger=True)\n",
    "        self.log('train_acc',self.train_acc.compute(),logger=True)\n",
    "        \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        _, _, answers, *batch_input = batch\n",
    "        logits = self(*batch_input)\n",
    "        loss = self.criterion(logits, answers)\n",
    "        acc = self.valid_acc(logits,answers)\n",
    "        return {'val_loss': loss,'n_samples':len(answers)}\n",
    "    \n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        loss=0\n",
    "        n_samples = 0\n",
    "        for step_out in val_step_outputs:\n",
    "            loss += step_out['val_loss']\n",
    "            n_samples += step_out['n_samples']\n",
    "        loss = loss/n_samples\n",
    "        val_acc = self.valid_acc.compute()\n",
    "        self.log('val_acc',val_acc,prog_bar = True,logger=True)\n",
    "        self.log('val_loss',loss,logger=True)\n",
    "        \n",
    "    def test_step(self,batch,batch_idx):\n",
    "        _, _, answers, *batch_input = batch\n",
    "        logits = self(*batch_input)\n",
    "        acc = self.test_acc(logits,answers)\n",
    "    \n",
    "    def test_epoch_end(self,test_step_outputs):\n",
    "        test_acc = self.test_acc.compute()\n",
    "        print(f\"The test accuracy is {test_acc}\")\n",
    "        self.log('test_acc',test_acc,logger=True)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreating papers results on MSVD-QA, MSRVTT-QA, TGIF-QA_FrameQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSVD-QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Checkpoint directory models_checkpoints/msvd/baseline exists and is not empty.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 1\n",
    "\n",
    "model_kwargs = {\n",
    "        'question_type': msvd_glove_data_module.question_type,\n",
    "        'vision_dim': 2048,\n",
    "        'module_dim': 512,\n",
    "        'word_dim': 300,\n",
    "        'k_max_frame_level': 16,\n",
    "        'k_max_clip_level': 8,\n",
    "        'spl_resolution': 1,\n",
    "        'vocab': msvd_glove_data_module.vocab\n",
    "    }\n",
    "model = HCRN_glove(\n",
    "    glove_matrix=msvd_glove_data_module.glove_matrix,\n",
    "    lr=0.0001,\n",
    "    model_kwargs=model_kwargs,\n",
    "    optimizer='Adam'\n",
    ")\n",
    "\n",
    "checkpoint_callback_val_acc = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_acc',\n",
    "    dirpath='models_checkpoints/msvd/baseline',\n",
    "    filename='msvd-base-{epoch:02d}-{val_acc:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='max',\n",
    ")\n",
    "wandb_logger = WandbLogger(name='HCRN-MSVD-base',project='video-qa-hcrn-recvis')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=max_epochs,\n",
    "    logger = wandb_logger,\n",
    "    callbacks =[checkpoint_callback_val_acc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | criterion | CrossEntropyLoss | 0     \n",
      "1 | train_acc | Accuracy         | 0     \n",
      "2 | valid_acc | Accuracy         | 0     \n",
      "3 | test_acc  | Accuracy         | 0     \n",
      "4 | model     | HCRNNetwork      | 43.7 M\n",
      "-----------------------------------------------\n",
      "43.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "43.7 M    Total params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading questions from data/msvd-qa/glove_question_embedding/msvd-qa_val_questions.pt\n",
      "loading appearance feature from data/msvd-qa/msvd-qa_appearance_feat.h5\n",
      "loading motion feature from data/msvd-qa/msvd-qa_motion_feat.h5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading questions from data/msvd-qa/glove_question_embedding/msvd-qa_train_questions.pt\n",
      "loading appearance feature from data/msvd-qa/msvd-qa_appearance_feat.h5\n",
      "loading motion feature from data/msvd-qa/msvd-qa_motion_feat.h5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4939760435854d9383892122a5ef1721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model,msvd_glove_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading questions from data/msvd-qa/glove_question_embedding/msvd-qa_test_questions.pt\n",
      "loading appearance feature from data/msvd-qa/msvd-qa_appearance_feat.h5\n",
      "loading motion feature from data/msvd-qa/msvd-qa_motion_feat.h5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58602c5a16954ca1a82d1655dd0a383f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Testing'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is 0.32553014159202576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_res = trainer.test(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({'test_acc':test_res[0]['test_acc']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/jupyter/video-qa-recvis/wandb/run-20210102_013312-2yuvqn87/files/msvd-base-epoch=00-val_acc=0.31-v1.ckpt']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.save(trainer.checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSRVTT-QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 25\n",
    "\n",
    "model_kwargs = {\n",
    "        'question_type': msrvtt_glove_data_module.question_type,\n",
    "        'vision_dim': 2048,\n",
    "        'module_dim': 512,\n",
    "        'word_dim': 300,\n",
    "        'k_max_frame_level': 16,\n",
    "        'k_max_clip_level': 8,\n",
    "        'spl_resolution': 1,\n",
    "        'vocab': msrvtt_glove_data_module.vocab\n",
    "    }\n",
    "model = HCRN_glove(\n",
    "    glove_matrix=msrvtt_glove_data_module.glove_matrix,\n",
    "    lr=0.0001, \n",
    "    model_kwargs=model_kwargs, \n",
    "    optimizer='Adam'\n",
    ")\n",
    "\n",
    "checkpoint_callback_val_acc = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_acc',\n",
    "    dirpath='models_checkpoints/msrvtt/baseline',\n",
    "    filename='msrvtt-base-{epoch:02d}-{val_acc:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='max',\n",
    ")\n",
    "wandb_logger = WandbLogger(name='HCRN-MSRVTT-base',project='video-qa-hcrn-recvis')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=max_epochs,\n",
    "    logger = wandb_logger,\n",
    "    callbacks =[checkpoint_callback_val_acc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model,msrvtt_glove_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = trainer.test(verbose=False)\n",
    "wandb.log({'test_acc':test_res[0]['test_acc']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.save(trainer.checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TGIF-QA FrameQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 25\n",
    "\n",
    "model_kwargs = {\n",
    "        'question_type': tgif_glove_data_module.question_type,\n",
    "        'vision_dim': 2048,\n",
    "        'module_dim': 512,\n",
    "        'word_dim': 300,\n",
    "        'k_max_frame_level': 16,\n",
    "        'k_max_clip_level': 8,\n",
    "        'spl_resolution': 1,\n",
    "        'vocab': tgif_glove_data_module.vocab\n",
    "    }\n",
    "model = HCRN_glove(\n",
    "    glove_matrix=tgif_glove_data_module.glove_matrix,\n",
    "    lr=0.0001, \n",
    "    model_kwargs=model_kwargs, \n",
    "    optimizer='Adam'\n",
    ")\n",
    "\n",
    "checkpoint_callback_val_acc = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_acc',\n",
    "    dirpath='models_checkpoints/tgif-qa_frameqa/baseline',\n",
    "    filename='tgif-qa_frameqa-base-{epoch:02d}-{val_acc:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='max',\n",
    ")\n",
    "wandb_logger = WandbLogger(name='HCRN-MSRVTT-base',project='video-qa-hcrn-recvis')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=max_epochs,\n",
    "    logger = wandb_logger,\n",
    "    callbacks =[checkpoint_callback_val_acc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model,tgif_glove_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = trainer.test(verbose=False)\n",
    "wandb.log({'test_acc':test_res[0]['test_acc']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.save(trainer.checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving using AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSVD-QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 25\n",
    "\n",
    "model_kwargs = {\n",
    "        'question_type': msvd_glove_data_module.question_type,\n",
    "        'vision_dim': 2048,\n",
    "        'module_dim': 512,\n",
    "        'word_dim': 300,\n",
    "        'k_max_frame_level': 16,\n",
    "        'k_max_clip_level': 8,\n",
    "        'spl_resolution': 1,\n",
    "        'vocab': msvd_glove_data_module.vocab\n",
    "    }\n",
    "model = HCRN_glove(\n",
    "    glove_matrix=msvd_glove_data_module.glove_matrix,\n",
    "    lr=0.0001,\n",
    "    model_kwargs=model_kwargs,\n",
    "    optimizer='AdamW'\n",
    ")\n",
    "\n",
    "checkpoint_callback_val_acc = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_acc',\n",
    "    dirpath='models_checkpoints/msvd/baseline',\n",
    "    filename='msvd-adamw-{epoch:02d}-{val_acc:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='max',\n",
    ")\n",
    "wandb_logger = WandbLogger(name='HCRN-MSVD-base',project='video-qa-hcrn-recvis')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=max_epochs,\n",
    "    logger = wandb_logger,\n",
    "    callbacks =[checkpoint_callback_val_acc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model,msvd_glove_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = trainer.test(verbose=False)\n",
    "wandb.log({'test_acc':test_res[0]['test_acc']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.save(trainer.checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSRVTT-QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Checkpoint directory models_checkpoints/msrvtt/baseline exists and is not empty.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 25\n",
    "\n",
    "model_kwargs = {\n",
    "        'question_type': msrvtt_glove_data_module.question_type,\n",
    "        'vision_dim': 2048,\n",
    "        'module_dim': 512,\n",
    "        'word_dim': 300,\n",
    "        'k_max_frame_level': 16,\n",
    "        'k_max_clip_level': 8,\n",
    "        'spl_resolution': 1,\n",
    "        'vocab': msrvtt_glove_data_module.vocab\n",
    "    }\n",
    "model = HCRN_glove(\n",
    "    glove_matrix=msrvtt_glove_data_module.glove_matrix,\n",
    "    lr=0.0001, \n",
    "    model_kwargs=model_kwargs, \n",
    "    optimizer='AdamW'\n",
    ")\n",
    "\n",
    "checkpoint_callback_val_acc = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_acc',\n",
    "    dirpath='models_checkpoints/msrvtt/baseline',\n",
    "    filename='msrvtt-adamw-{epoch:02d}-{val_acc:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='max',\n",
    ")\n",
    "wandb_logger = WandbLogger(name='HCRN-MSRVTT-base',project='video-qa-hcrn-recvis')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=max_epochs,\n",
    "    logger = wandb_logger,\n",
    "    callbacks =[checkpoint_callback_val_acc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | criterion | CrossEntropyLoss | 0     \n",
      "1 | train_acc | Accuracy         | 0     \n",
      "2 | valid_acc | Accuracy         | 0     \n",
      "3 | test_acc  | Accuracy         | 0     \n",
      "4 | model     | HCRNNetwork      | 48.5 M\n",
      "-----------------------------------------------\n",
      "48.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "48.5 M    Total params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading questions from data/msrvtt-qa/glove_question_embedding/msrvtt-qa_val_questions.pt\n",
      "loading appearance feature from data/msrvtt-qa/msrvtt-qa_appearance_feat.h5\n",
      "loading motion feature from data/msrvtt-qa/msrvtt-qa_motion_feat.h5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading questions from data/msrvtt-qa/glove_question_embedding/msrvtt-qa_train_questions.pt\n",
      "loading appearance feature from data/msrvtt-qa/msrvtt-qa_appearance_feat.h5\n",
      "loading motion feature from data/msrvtt-qa/msrvtt-qa_motion_feat.h5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c1b264f11b48d297b6c6fe2592ddde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model,msrvtt_glove_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = trainer.test(verbose=False)\n",
    "wandb.log({'test_acc':test_res[0]['test_acc']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.save(trainer.checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TGIF-QA FrameQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 25\n",
    "\n",
    "model_kwargs = {\n",
    "        'question_type': tgif_glove_data_module.question_type,\n",
    "        'vision_dim': 2048,\n",
    "        'module_dim': 512,\n",
    "        'word_dim': 300,\n",
    "        'k_max_frame_level': 16,\n",
    "        'k_max_clip_level': 8,\n",
    "        'spl_resolution': 1,\n",
    "        'vocab': tgif_glove_data_module.vocab\n",
    "    }\n",
    "model = HCRN_glove(\n",
    "    glove_matrix=tgif_glove_data_module.glove_matrix,\n",
    "    lr=0.0001, \n",
    "    model_kwargs=model_kwargs, \n",
    "    optimizer='AdamW'\n",
    ")\n",
    "\n",
    "checkpoint_callback_val_acc = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_acc',\n",
    "    dirpath='models_checkpoints/tgif-qa_frameqa/baseline',\n",
    "    filename='tgif-qa_frameqa-base-{epoch:02d}-{val_acc:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='max',\n",
    ")\n",
    "wandb_logger = WandbLogger(name='HCRN-MSRVTT-base',project='video-qa-hcrn-recvis')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=max_epochs,\n",
    "    logger = wandb_logger,\n",
    "    callbacks =[checkpoint_callback_val_acc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model,tgif_glove_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = trainer.test(verbose=False)\n",
    "wandb.log({'test_acc':test_res[0]['test_acc']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.save(trainer.checkpoint_callback.best_model_path)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
